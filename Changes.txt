Na gałęzi feature/v0.1.1/remote-training
Twoja gałąź jest na bieżąco z „origin/feature/v0.1.1/remote-training”.

Zmiany do złożenia:
  (użyj „git restore --staged <plik>...”, aby wycofać)
	zmieniono:       configs/train_kadid10k_vit_base_patch16_224_baseline.yaml
	zmieniono:       configs/train_live_vit_base_patch16_224_baseline.yaml
	zmieniono:       configs/train_tid2008_vit_base_patch16_224_baseline.yaml
	zmieniono:       configs/train_tid2013_vit_base_patch16_224_baseline.yaml
	nowy plik:       scripts/download_datasets.py
	zmieniono:       src/cli/run_evaluation_cli.py
	zmieniono:       src/datasets/base_dataset.py
	zmieniono:       src/datasets/file_map.py
	zmieniono:       src/experiments/experiments.py
	zmieniono:       src/inference/predictor.py
	zmieniono:       src/utils/configs.py
	zmieniono:       src/utils/paths.py
	nowy plik:       src/utils/urls.py

Nieśledzone pliki:
  (użyj „git add <plik>...”, żeby uwzględnić, co zostanie złożone)
	Changes.txt

diff --git a/configs/train_kadid10k_vit_base_patch16_224_baseline.yaml b/configs/train_kadid10k_vit_base_patch16_224_baseline.yaml
index 24529b8..242fb5b 100644
--- a/configs/train_kadid10k_vit_base_patch16_224_baseline.yaml
+++ b/configs/train_kadid10k_vit_base_patch16_224_baseline.yaml
@@ -35,13 +35,13 @@ model:
     keep_original_aspect_ratio: true
   embedding_dimension: 768
   output:
-    type: "inverted_normalized_dmos"
+    type: "unified_quality_score"
     min: 0
     max: 1
 
 training:
   quality_label:
-    type: "inverted_normalized_dmos"
+    type: "unified_quality_score"
     min: 0
     max: 1
   splits:
@@ -50,7 +50,7 @@ training:
     test: 0.2
     random_seed: 42
   batch_size: 8
-  num_of_epochs: 5
+  num_of_epochs: 100
   early_stopping:
     enabled: false
     max_epochs_without_improvement: 5
@@ -61,7 +61,7 @@ training:
 
 checkpointing:
   enabled: true
-  save_every_n_epochs: 1
+  save_every_n_epochs: 20
   save_last_epoch: true
   save_best_epoch: true
 
diff --git a/configs/train_live_vit_base_patch16_224_baseline.yaml b/configs/train_live_vit_base_patch16_224_baseline.yaml
index 0acfc04..39e2bfc 100644
--- a/configs/train_live_vit_base_patch16_224_baseline.yaml
+++ b/configs/train_live_vit_base_patch16_224_baseline.yaml
@@ -35,13 +35,13 @@ model:
     keep_original_aspect_ratio: true
   embedding_dimension: 768
   output:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
 
 training:
   quality_label:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
   splits:
@@ -50,7 +50,7 @@ training:
     test: 0.2
     random_seed: 42
   batch_size: 8
-  num_of_epochs: 5
+  num_of_epochs: 100
   early_stopping:
     enabled: false
     max_epochs_without_improvement: 5
@@ -61,7 +61,7 @@ training:
 
 checkpointing:
   enabled: true
-  save_every_n_epochs: 1
+  save_every_n_epochs: 20
   save_last_epoch: true
   save_best_epoch: true
 
diff --git a/configs/train_tid2008_vit_base_patch16_224_baseline.yaml b/configs/train_tid2008_vit_base_patch16_224_baseline.yaml
index aca14d6..cea68bc 100644
--- a/configs/train_tid2008_vit_base_patch16_224_baseline.yaml
+++ b/configs/train_tid2008_vit_base_patch16_224_baseline.yaml
@@ -35,13 +35,13 @@ model:
     keep_original_aspect_ratio: true
   embedding_dimension: 768
   output:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
 
 training:
   quality_label:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
   splits:
@@ -50,7 +50,7 @@ training:
     test: 0.2
     random_seed: 42
   batch_size: 8
-  num_of_epochs: 5
+  num_of_epochs: 100
   early_stopping:
     enabled: false
     max_epochs_without_improvement: 5
@@ -61,7 +61,7 @@ training:
 
 checkpointing:
   enabled: true
-  save_every_n_epochs: 1
+  save_every_n_epochs: 20
   save_last_epoch: true
   save_best_epoch: true
 
diff --git a/configs/train_tid2013_vit_base_patch16_224_baseline.yaml b/configs/train_tid2013_vit_base_patch16_224_baseline.yaml
index c20c66f..366efd7 100644
--- a/configs/train_tid2013_vit_base_patch16_224_baseline.yaml
+++ b/configs/train_tid2013_vit_base_patch16_224_baseline.yaml
@@ -35,13 +35,13 @@ model:
     keep_original_aspect_ratio: true
   embedding_dimension: 768
   output:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
 
 training:
   quality_label:
-    type: "normalized_mos"
+    type: "unified_quality_score"
     min: 0
     max: 1
   splits:
@@ -50,7 +50,7 @@ training:
     test: 0.2
     random_seed: 42
   batch_size: 8
-  num_of_epochs: 5
+  num_of_epochs: 100
   early_stopping:
     enabled: false
     max_epochs_without_improvement: 5
@@ -61,7 +61,7 @@ training:
 
 checkpointing:
   enabled: true
-  save_every_n_epochs: 1
+  save_every_n_epochs: 20
   save_last_epoch: true
   save_best_epoch: true
 
diff --git a/scripts/download_datasets.py b/scripts/download_datasets.py
new file mode 100644
index 0000000..c5d0db9
--- /dev/null
+++ b/scripts/download_datasets.py
@@ -0,0 +1,44 @@
+from pathlib import Path
+
+from src.utils.urls import (
+    download_file,
+    DATASET_TID2008_URL,
+    DATASET_TID2013_URL,
+    DATASET_KADID10K_URL
+)
+from src.utils.paths import (
+    DATASET_TID2008_ARCHIVE_PATH,
+    DATASET_TID2013_ARCHIVE_PATH,
+    DATASET_KADID10K_ARCHIVE_PATH, PROJECT_ROOT_PATH
+)
+
+
+def download_dataset(file_url: str, destination_file_path: Path):
+    if not destination_file_path.exists():
+        print(f"\nRozpoczynam pobieranie datasetu: {file_url}")
+
+        download_file(
+            file_url=file_url,
+            destination_file_path=destination_file_path
+        )
+
+        print(f"Dataset zapisano w: {destination_file_path}")
+    else:
+        print(f"\nZnaleziono plik `{destination_file_path}`. Pomijam pobieranie...")
+
+
+if __name__ == '__main__':
+    download_dataset(
+        file_url=DATASET_TID2008_URL,
+        destination_file_path=DATASET_TID2008_ARCHIVE_PATH
+    )
+
+    download_dataset(
+        file_url=DATASET_TID2013_URL,
+        destination_file_path=DATASET_TID2013_ARCHIVE_PATH
+    )
+
+    download_dataset(
+        file_url=DATASET_KADID10K_URL,
+        destination_file_path=DATASET_KADID10K_ARCHIVE_PATH
+    )
diff --git a/src/cli/run_evaluation_cli.py b/src/cli/run_evaluation_cli.py
index 18be1f8..b77aa88 100644
--- a/src/cli/run_evaluation_cli.py
+++ b/src/cli/run_evaluation_cli.py
@@ -46,7 +46,6 @@ class EvaluationCliCommand(BaseCliCommand[EvaluationCliArgs]):
         checkpoint_name = parsed_namespace.checkpoint_name
 
         validate_split_name_arg(experiment_path=experiment_path, split_name=split_name)
-        resolve_checkpoint_path(experiment_path=experiment_path, checkpoint_name=checkpoint_name)
 
         return EvaluationCliArgs(
             experiment_path=experiment_path,
diff --git a/src/datasets/base_dataset.py b/src/datasets/base_dataset.py
index 63677aa..5c9cb90 100644
--- a/src/datasets/base_dataset.py
+++ b/src/datasets/base_dataset.py
@@ -10,7 +10,7 @@ from torchvision import transforms
 from src.datasets.file_map import FileMap
 from src.utils.data_types import Label, Config, UnifiedQualityScore
 from src.utils.image_preprocessing import resize
-from src.utils.paths import PROJECT_ROOT
+from src.utils.paths import PROJECT_ROOT_PATH
 
 
 LabelsContainerType = TypeVar('LabelsContainerType')
@@ -19,7 +19,7 @@ class BaseDataset(ABC, TorchDataset, Generic[LabelsContainerType]):
     def __init__(self, config):
         self._config = config
 
-        self._labels_path = PROJECT_ROOT / config['dataset']['labels_path']
+        self._labels_path = PROJECT_ROOT_PATH / config['dataset']['labels_path']
 
         self._reference_images_path = Path(config['dataset']['images']['reference']['path'])
         self._distorted_images_path = Path(config['dataset']['images']['distorted']['path'])
diff --git a/src/datasets/file_map.py b/src/datasets/file_map.py
index b41f382..5f0e86f 100644
--- a/src/datasets/file_map.py
+++ b/src/datasets/file_map.py
@@ -1,6 +1,6 @@
 from pathlib import Path
 
-from src.utils.paths import PROJECT_ROOT
+from src.utils.paths import PROJECT_ROOT_PATH
 
 
 class FileMap:
@@ -17,7 +17,7 @@ class FileMap:
 
     @files_directory_path.setter
     def files_directory_path(self, new_directory_path: Path):
-        new_directory_path = PROJECT_ROOT / new_directory_path
+        new_directory_path = PROJECT_ROOT_PATH / new_directory_path
 
         if not new_directory_path or not new_directory_path.exists():
             raise FileNotFoundError(
diff --git a/src/experiments/experiments.py b/src/experiments/experiments.py
index e2b099f..d7d437c 100644
--- a/src/experiments/experiments.py
+++ b/src/experiments/experiments.py
@@ -46,7 +46,7 @@ def _create_directories(experiment_path: Path) -> None:
     ]
 
     for directory in directories:
-        directory.mkdir(parents=True, exist_ok=True)
+        directory.mkdir(parents=False, exist_ok=False)
 
 
 def _create_empty_files(experiment_path: Path) -> None:
@@ -126,6 +126,8 @@ def create_experiment(
 
         print(f"\nPoprawnie utworzono eksperyment `{experiment_name}`!")
 
+        return experiment_path
+
     except FileExistsError:
         raise FileExistsError(
             f'Error: Eksperyment `{experiment_name}` dla datasetu `{dataset_name}` już istnieje!\n'
@@ -143,5 +145,3 @@ def create_experiment(
 
     finally:
         print(f'Ścieżka: {experiment_path}\n')
-
-        return experiment_path
diff --git a/src/inference/predictor.py b/src/inference/predictor.py
index 1dbabf5..5d3cd4c 100644
--- a/src/inference/predictor.py
+++ b/src/inference/predictor.py
@@ -47,7 +47,8 @@ class Predictor:
 
         print(
             f"\n[Predictor] Rozpoczęto ładowanie checkpointu:\n"
-            f"    Nazwa checkpointu: {checkpoint_name}\n"
+            f"    Ścieżka eksperymentu: {experiment_path}\n"
+            f"    Nazwa checkpointu: `{checkpoint_name}`\n"
             f"    Nazwa configu: `{self.config['config_name']}`"
         )
 
diff --git a/src/utils/configs.py b/src/utils/configs.py
index cbab235..30edb48 100644
--- a/src/utils/configs.py
+++ b/src/utils/configs.py
@@ -4,7 +4,7 @@ from typing import Any
 import yaml
 
 from src.utils.data_types import Config
-from src.utils.paths import PROJECT_ROOT
+from src.utils.paths import PROJECT_ROOT_PATH
 
 
 def load_config(config_path: Path, check_consistency: bool) -> Config:
@@ -143,7 +143,7 @@ def _check_dataset_section(dataset: dict[str, Any]) -> None:
 
 
     # --------------- images.reference.path ---------------
-    reference_images_path = PROJECT_ROOT / reference['path']
+    reference_images_path = PROJECT_ROOT_PATH / reference['path']
     if not reference_images_path.exists() or not reference_images_path.is_dir():
         raise FileNotFoundError(
             f"Error: `dataset.images.reference.path` nie istnieje lub nie jest katalogiem!\n"
@@ -169,7 +169,7 @@ def _check_dataset_section(dataset: dict[str, Any]) -> None:
     )
 
     # --------------- images.distorted.path ---------------
-    distorted_images_path = PROJECT_ROOT / distorted['path']
+    distorted_images_path = PROJECT_ROOT_PATH / distorted['path']
     if not distorted_images_path.exists() or not distorted_images_path.is_dir():
         raise FileNotFoundError(
             f"Error: `dataset.images.distorted.path` nie istnieje lub nie jest katalogiem!\n"
@@ -214,7 +214,7 @@ def _check_dataset_section(dataset: dict[str, Any]) -> None:
 
 
     # --------------- labels_path ---------------
-    labels_path = PROJECT_ROOT / dataset['labels_path']
+    labels_path = PROJECT_ROOT_PATH / dataset['labels_path']
     if not labels_path.exists() or not labels_path.is_file():
         raise FileNotFoundError(
             f"Error: `dataset.labels_path` nie istnieje lub nie jest plikiem!\n"
@@ -329,7 +329,7 @@ def _check_model_section(model: dict[str, Any]) -> None:
 
 
     # --------------- output.type ---------------
-    if output_config['type'] not in ('normalized_mos', 'inverted_normalized_dmos'):
+    if output_config['type'] != 'unified_quality_score':
         raise ValueError('Error: `model.output.type` musi mieć wartość `normalized_mos` albo `inverted_normalized_dmos`!')
 
 
@@ -518,12 +518,7 @@ def _check_cross_section_consistency(config: dict[str, Any]) -> None:
     model_output_type = config['model']['output']['type']
     training_quality_type = config['training']['quality_label']['type']
 
-    expected_output_type_by_dataset_label_type = {
-        'mos': 'normalized_mos',
-        'dmos': 'inverted_normalized_dmos',
-    }
-
-    expected_type = expected_output_type_by_dataset_label_type[dataset_quality_type]
+    expected_type = 'unified_quality_score'
 
     if model_output_type != expected_type:
         raise ValueError(
diff --git a/src/utils/paths.py b/src/utils/paths.py
index 8e35b39..f06fc58 100644
--- a/src/utils/paths.py
+++ b/src/utils/paths.py
@@ -1,18 +1,35 @@
 from pathlib import Path
 
+from src.utils.urls import (
+    get_file_name,
+    DATASET_TID2008_URL,
+    DATASET_TID2013_URL,
+    DATASET_KADID10K_URL
+)
 
-PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
+PROJECT_ROOT_PATH = Path(__file__).resolve().parent.parent.parent
 
-CONFIGS_PATH = PROJECT_ROOT / 'configs/'
+CONFIGS_PATH = PROJECT_ROOT_PATH / 'configs/'
 CONFIG_TRAIN_KADID10K_BASELINE_PATH = CONFIGS_PATH / 'train_kadid10k_vit_base_patch16_224_baseline.yaml'
 CONFIG_TRAIN_TID2008_BASELINE_PATH = CONFIGS_PATH / 'train_tid2008_vit_base_patch16_224_baseline.yaml'
 CONFIG_TRAIN_TID2013_BASELINE_PATH = CONFIGS_PATH / 'train_tid2013_vit_base_patch16_224_baseline.yaml'
 CONFIG_TRAIN_LIVE_BASELINE_PATH = CONFIGS_PATH / 'train_live_vit_base_patch16_224_baseline.yaml'
 
-EXPERIMENTS_PATH = PROJECT_ROOT / 'experiments/'
+EXPERIMENTS_PATH = PROJECT_ROOT_PATH / 'experiments/'
 EXPERIMENTS_KADID10K_PATH = EXPERIMENTS_PATH / 'kadid10k/'
 EXPERIMENTS_TID2008_PATH = EXPERIMENTS_PATH / 'tid2008/'
 EXPERIMENTS_TID2013_PATH = EXPERIMENTS_PATH / 'tid2013/'
 EXPERIMENTS_LIVE_PATH = EXPERIMENTS_PATH / 'live/'
 
-SRC_PATH = PROJECT_ROOT / 'src/'
+DATASETS_PATH = PROJECT_ROOT_PATH / 'datasets/'
+
+DATASET_TID2008_ARCHIVE_FILE_NAME = get_file_name(url=DATASET_TID2008_URL)
+DATASET_TID2013_ARCHIVE_FILE_NAME = get_file_name(url=DATASET_TID2013_URL)
+DATASET_KADID10K_ARCHIVE_FILE_NAME = get_file_name(url=DATASET_KADID10K_URL)
+
+DATASET_TID2008_ARCHIVE_PATH = DATASETS_PATH / DATASET_TID2008_ARCHIVE_FILE_NAME
+DATASET_TID2013_ARCHIVE_PATH = DATASETS_PATH / DATASET_TID2013_ARCHIVE_FILE_NAME
+DATASET_KADID10K_ARCHIVE_PATH = DATASETS_PATH / DATASET_KADID10K_ARCHIVE_FILE_NAME
+
+SRC_PATH = PROJECT_ROOT_PATH / 'src/'
+
diff --git a/src/utils/urls.py b/src/utils/urls.py
new file mode 100644
index 0000000..a00a992
--- /dev/null
+++ b/src/utils/urls.py
@@ -0,0 +1,65 @@
+from pathlib import Path, PurePosixPath
+from urllib.request import Request, urlopen
+from urllib.error import HTTPError, URLError
+from urllib.parse import urlparse, unquote
+import shutil
+
+
+def download_file(
+        file_url: str,
+        destination_file_path: Path,
+        overwrite: bool = False,
+        timeout_seconds: int = 60
+) -> Path:
+    destination_file_path.parent.mkdir(parents=True, exist_ok=True)
+
+    if not overwrite and destination_file_path.exists() and destination_file_path.is_file():
+        raise FileExistsError(
+            f"Error: Pobierany plik istnieje już we wskazanej lokalizacji!\n"
+            f"Jeśli chcesz go nadpisać, dodaj parametr `overwrite=True`.\n"
+            f"Ścieżka: {destination_file_path}"
+        )
+
+    request = Request(
+        url=file_url,
+        headers={"User-Agent": "Mozilla/5.0 (compatible; dataset-downloader/1.0)"},
+    )
+
+    try:
+        with urlopen(request, timeout=timeout_seconds) as response_stream:
+            # Otwarcie docelowego pliku w trybie zapisu binarnego
+            with open(destination_file_path, "wb") as output_file:
+                # Kopiowanie zwróconych bajtów do docelowego pliku poprzez buforowanie / streamowanie
+                shutil.copyfileobj(response_stream, output_file)  # type: ignore[arg-type]
+
+    except HTTPError as http_error:
+        # HTTPError zawiera kody statusów i wiadomości HTTP (np. 404, 403)
+        raise RuntimeError(
+            f"Error: Błąd HTTP w trakcie pobierania pliku!\n"
+            f"    status={http_error.code}\n"
+            f"    reason={http_error.reason}\n"
+            f"    url={file_url}"
+        ) from http_error
+
+    except URLError as url_error:
+        # URLError obejmuje problemy z DNS, odmowę połączenia, brak dostępu do Internetu itp.
+        raise RuntimeError(
+            f"Error: Błąd Sieci albo URL w trakcie pobierania pliku!\n"
+            f"    url={file_url}\n"
+            f"    error={url_error}"
+        ) from url_error
+
+    return destination_file_path
+
+
+def get_file_name(url: str) -> str:
+    parsed_url = urlparse(url)
+    decoded_path = unquote(parsed_url.path)
+    file_name = PurePosixPath(decoded_path).name
+
+    return file_name
+
+
+DATASET_TID2008_URL = 'http://www.ponomarenko.info/tid/tid2008.rar'
+DATASET_TID2013_URL = 'http://www.ponomarenko.info/tid2013/tid2013.rar'
+DATASET_KADID10K_URL = 'https://datasets.vqa.mmsp-kn.de/archives/kadid10k.zip'
