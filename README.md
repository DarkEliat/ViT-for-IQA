# ViT-for-IQA

**Vision Transformer for Full-Reference Image Quality Assessment**

A deep learning-based image quality assessment system using Vision Transformer (ViT) architecture. This project implements a full-reference IQA model that predicts perceptual quality scores by comparing distorted images against their pristine references.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Supported Datasets](#supported-datasets)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Creating Experiments](#creating-experiments)
  - [Training](#training)
  - [Evaluation](#evaluation)
  - [Inference](#inference)
- [Evaluation Metrics](#evaluation-metrics)
- [Results](#results)
- [Technical Details](#technical-details)
- [Requirements](#requirements)
- [License](#license)
- [Author](#author)

---

## ğŸ¯ Overview

Image Quality Assessment (IQA) is crucial in many computer vision applications. This project leverages the power of Vision Transformers to accurately predict perceptual quality scores for distorted images in a full-reference (FR-IQA) setting.

**What is Full-Reference IQA?**

- Takes both a pristine reference image and its distorted version as input
- Predicts a quality score that correlates with human perception
- Useful for evaluating compression algorithms, transmission systems, and image processing pipelines

---

## âœ¨ Key Features

- **ğŸ—ï¸ Modern Architecture**: Utilizes pretrained Vision Transformer (ViT) backbones from `timm`
- **ğŸ“Š Multiple Datasets**: Supports KADID-10k, TID2008, TID2013, and LIVE databases
- **ğŸ”¬ Rigorous Evaluation**: Implements standard IQA metrics (PLCC, SRCC, KRCC)
- **âš™ï¸ Flexible Configuration**: YAML-based configuration system with extensive validation
- **ğŸ’¾ Experiment Management**: Organized experiment structure with automatic checkpointing
- **ğŸ”„ Resume Training**: Intelligent checkpoint loading and training resumption
- **ğŸ“ˆ Logging**: TensorBoard integration for training visualization
- **ğŸ¯ Smart Data Splitting**: Reference-based splitting to prevent data leakage

---

## ğŸ›ï¸ Architecture

The model architecture consists of:

1. **Backbone**: Pretrained Vision Transformer (default: `vit_base_patch16_224`)

   - Processes both reference and distorted images independently
   - Extracts 768-dimensional embeddings for each image
2. **Regression Head**:

   - Concatenates embeddings: `[reference_emb, distorted_emb]` â†’ 1536 dimensions
   - Fully-connected layers: `Linear(1536â†’512) â†’ ReLU â†’ Linear(512â†’1)`
   - Outputs a single quality score
3. **Training**:

   - Loss function: Mean Squared Error (MSE)
   - Optimizer: Adam
   - Configurable learning rate, batch size, and epochs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reference Image â”‚     â”‚ Distorted Image â”‚
â”‚   (224Ã—224Ã—3)   â”‚     â”‚   (224Ã—224Ã—3)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ViT Backbone        â”‚
         â”‚   (Shared Weights)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         [768-d] â”‚ [768-d]
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Concatenation       â”‚
         â”‚      (1536-d)         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Regression Head     â”‚
         â”‚   FC(1536â†’512â†’1)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Quality Score [0,1]  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Supported Datasets

| Dataset             | Reference Images | Distorted Images | Score Type | Range    |
| ------------------- | ---------------- | ---------------- | ---------- | -------- |
| **KADID-10k** | 81               | 10,125           | DMOS       | [1, 5]   |
| **TID2008**   | 25               | 1,700            | MOS        | [0, 9]   |
| **TID2013**   | 25               | 3,000            | MOS        | [0, 9]   |
| **LIVE**      | 80               | 320              | MOS        | [1, 100] |

**Note**: All scores are automatically normalized to [0, 1] for training.

---

## ğŸš€ Installation

### Prerequisites

- Python 3.12.x
- CUDA-capable GPU (recommended)
- Poetry (package manager)

### Setup

1. **Clone the repository**:

   ```bash
   git clone https://github.com/yourusername/ViT-for-IQA.git
   cd ViT-for-IQA
   ```
2. **Install dependencies**:

   ```bash
   poetry install
   ```
3. **Activate the environment**:

   ```bash
   poetry env activate
   ```
4. **Download datasets** (manual):

   - Place datasets in the `datasets/` directory
   - Expected structure:
     ```
     datasets/
     â”œâ”€â”€ kadid10k/
     â”‚   â”œâ”€â”€ images/
     â”‚   â””â”€â”€ dmos.csv
     â”œâ”€â”€ tid2008/
     â”‚   â”œâ”€â”€ reference_images/
     â”‚   â”œâ”€â”€ distorted_images/
     â”‚   â””â”€â”€ mos_with_names.txt
     â”œâ”€â”€ tid2013/
     â”‚   â”œâ”€â”€ reference_images/
     â”‚   â”œâ”€â”€ distorted_images/
     â”‚   â””â”€â”€ mos_with_names.txt
     â””â”€â”€ live/
         â”œâ”€â”€ Images/
         â””â”€â”€ MOS.mat
     ```

---

## âš¡ Quick Start

### 1. Create an Experiment

```bash
python scripts/create_experiment.py
```

This will:

- Prompt you to select a dataset configuration
- Ask for an experiment name
- Generate train/validation/test splits
- Create experiment directory structure

### 2. Train the Model

Edit `scripts/run_training.py` to point to your experiment:

```python
from src.training.trainer import Trainer
from src.utils.paths import EXPERIMENTS_LIVE_PATH

trainer = Trainer(experiment_path=(EXPERIMENTS_LIVE_PATH / 'my_experiment'))
trainer.train()
```

Run training:

```bash
python scripts/run_training.py
```

### 3. Evaluate

Edit `scripts/run_evaluation.py`:

```python
from src.evaluation.evaluator import Evaluator
from src.utils.paths import EXPERIMENTS_LIVE_PATH

evaluator = Evaluator(
    experiment_path=(EXPERIMENTS_LIVE_PATH / 'my_experiment'),
    split_name='test',
    checkpoint_name='last.pth'
)

results = evaluator.evaluate(
    apply_nonlinear_regression_for_plcc=True,
    save_outputs=True
)
```

Run evaluation:

```bash
python scripts/run_evaluation.py
```

---

## ğŸ“‚ Project Structure

```
ViT-for-IQA/
â”œâ”€â”€ configs/                              # Dataset configurations
â”‚   â”œâ”€â”€ train_kadid10k_vit_base_patch16_224_baseline.yaml
â”‚   â”œâ”€â”€ train_live_vit_base_patch16_224_baseline.yaml
â”‚   â”œâ”€â”€ train_tid2008_vit_base_patch16_224_baseline.yaml
â”‚   â””â”€â”€ train_tid2013_vit_base_patch16_224_baseline.yaml
â”œâ”€â”€ datasets/                             # Dataset storage (gitignored)
â”œâ”€â”€ experiments/                          # Experiment outputs (gitignored)
â”‚   â””â”€â”€ {dataset_name}/
â”‚       â””â”€â”€ {experiment_name}/
â”‚           â”œâ”€â”€ config.yaml               # Experiment configuration
â”‚           â”œâ”€â”€ checkpoints/              # Model checkpoints (.pth)
â”‚           â”œâ”€â”€ logs/
â”‚           â”‚   â”œâ”€â”€ tensorboard/          # TensorBoard logs
â”‚           â”‚   â””â”€â”€ train.log             # Training logs
â”‚           â”œâ”€â”€ splits/                   # Dataset split indices
â”‚           â”‚   â”œâ”€â”€ train_indices.csv
â”‚           â”‚   â”œâ”€â”€ validation_indices.csv
â”‚           â”‚   â””â”€â”€ test_indices.csv
â”‚           â”œâ”€â”€ metrics.json              # Evaluation metrics
â”‚           â”œâ”€â”€ metrics.csv
â”‚           â””â”€â”€ summary.md                # Experiment summary
â”œâ”€â”€ scripts/                              # Executable scripts
â”‚   â”œâ”€â”€ create_experiment.py              # Create new experiment
â”‚   â”œâ”€â”€ run_training.py                   # Train model
â”‚   â”œâ”€â”€ run_evaluation.py                 # Evaluate model
â”‚   â””â”€â”€ run_prediction.py                 # Run inference
â”œâ”€â”€ src/                                  # Source code
â”‚   â”œâ”€â”€ datasets/                         # Dataset loaders
â”‚   â”‚   â”œâ”€â”€ base_dataset.py               # Abstract base class
â”‚   â”‚   â”œâ”€â”€ factory.py                    # Dataset factory
â”‚   â”‚   â”œâ”€â”€ file_map.py                   # File mapping utility
â”‚   â”‚   â”œâ”€â”€ kadid_dataset.py              # KADID-10k loader
â”‚   â”‚   â”œâ”€â”€ live_dataset.py               # LIVE loader
â”‚   â”‚   â”œâ”€â”€ tid_dataset.py                # TID2008/TID2013 loader
â”‚   â”‚   â””â”€â”€ splits.py                     # Data splitting logic
â”‚   â”œâ”€â”€ evaluation/                       # Evaluation tools
â”‚   â”‚   â”œâ”€â”€ correlation_metrics.py        # IQA metrics (PLCC, SRCC, KRCC)
â”‚   â”‚   â””â”€â”€ evaluator.py                  # Evaluation pipeline
â”‚   â”œâ”€â”€ inference/                        # Inference tools
â”‚   â”‚   â””â”€â”€ predictor.py                  # Prediction pipeline
â”‚   â”œâ”€â”€ models/                           # Model architectures
â”‚   â”‚   â””â”€â”€ vit_regressor.py              # ViT-based regressor
â”‚   â”œâ”€â”€ training/                         # Training logic
â”‚   â”‚   â””â”€â”€ trainer.py                    # Training pipeline
â”‚   â””â”€â”€ utils/                            # Utilities
â”‚       â”œâ”€â”€ checkpoints.py                # Checkpoint management
â”‚       â”œâ”€â”€ configs.py                    # Configuration validation
â”‚       â”œâ”€â”€ data_types.py                 # Type definitions
â”‚       â”œâ”€â”€ image_preprocessing.py        # Image preprocessing
â”‚       â”œâ”€â”€ paths.py                      # Path constants
â”‚       â””â”€â”€ quality_scores.py             # Score normalization
â”œâ”€â”€ pyproject.toml                        # Poetry dependencies
â”œâ”€â”€ TODO.md                               # Development roadmap
â””â”€â”€ README.md                             # This file
```

---

## âš™ï¸ Configuration

Configuration files are in YAML format and define all experiment parameters.

### Key Configuration Sections

```yaml
config_name: "live_vit_base_patch16_224_baseline"

app:
  version: "0.1.0"

dataset:
  name: "live"                            # Dataset identifier
  representative_name: "LIVE Wild Compressed Picture Quality Database"
  images:
    reference:
      path: "datasets/live/Images/"
      count: 80
    distorted:
      path: "datasets/live/Images/"
      count: 320
  quality_label:
    type: "mos"                           # "mos" or "dmos"
    min: 1
    max: 100
  labels_path: "datasets/live/MOS.mat"

model:
  name: "vit_base_patch16_224"            # Model from timm
  input:
    image_size:
      width: 224
      height: 224
    keep_original_aspect_ratio: true
  embedding_dimension: 768
  output:
    type: "normalized_mos"                # Output normalization
    min: 0
    max: 1

training:
  splits:
    train: 0.6
    validation: 0.2
    test: 0.2
    random_seed: 42
  batch_size: 8
  num_of_epochs: 5
  learning_rate: 0.0001
  device: "cuda"
  num_of_workers: 4
  early_stopping:
    enabled: false
    max_epochs_without_improvement: 5
    min_improvement_delta: 0.001

checkpointing:
  enabled: true
  save_every_n_epochs: 1
  save_last_epoch: true
  save_best_epoch: true

logging:
  tensorboard: true
```

### Configuration Validation

The system performs extensive validation:

- âœ… File and directory existence checks
- âœ… Value range validation
- âœ… Cross-section consistency (e.g., MOS â†’ normalized_mos)
- âœ… Type checking for all parameters

---

## ğŸ“– Usage

### Creating Experiments

The experiment creation script provides an interactive interface:

```bash
python scripts/create_experiment.py
```

**Options**:

1. Create from scratch using a global config file
2. Create from existing checkpoint (for fine-tuning)

**What it does**:

- Generates reference-based train/val/test splits
- Creates directory structure
- Copies configuration
- Initializes checkpoint (if specified)

### Training

The `Trainer` class handles the complete training pipeline:

```python
from pathlib import Path
from src.training.trainer import Trainer

# Initialize trainer
trainer = Trainer(experiment_path=Path("experiments/live/my_experiment"))

# Start/resume training
trainer.train()
```

**Features**:

- Automatic checkpoint resumption
- TensorBoard logging
- Validation after each epoch
- Configurable checkpoint saving

**Training outputs**:

- `checkpoints/last.pth` - Latest checkpoint
- `checkpoints/epoch_N.pth` - Periodic checkpoints
- `checkpoints/best.pth` - Best performing checkpoint
- `logs/tensorboard/` - TensorBoard logs

### Evaluation

The `Evaluator` class computes IQA metrics:

```python
from src.evaluation.evaluator import Evaluator

evaluator = Evaluator(
    experiment_path=Path("experiments/live/my_experiment"),
    split_name='test',                    # 'train', 'validation', or 'test'
    checkpoint_name='last.pth'
)

results = evaluator.evaluate(
    apply_nonlinear_regression_for_plcc=True,  # Fit 5-parameter logistic
    save_outputs=True                          # Save metrics to files
)

print(f"PLCC: {results.plcc:.4f}")
print(f"SRCC: {results.srcc:.4f}")
print(f"KRCC: {results.krcc:.4f}")
```

### Inference

The `Predictor` class enables predictions on custom data:

```python
from src.inference.predictor import Predictor

predictor = Predictor(
    experiment_path=Path("experiments/live/my_experiment"),
    checkpoint_name='last.pth'
)

# Predict on training dataset
predictions = predictor.predict_on_training_dataset()

# Predict with custom DataLoader
from torch.utils.data import DataLoader
custom_loader = DataLoader(...)
predictions = predictor.predict(data_loader=custom_loader)
```

---

## ğŸ“Š Evaluation Metrics

### Correlation Metrics

1. **PLCC (Pearson Linear Correlation Coefficient)**

   - Measures linear correlation with human perception
   - Optional 5-parameter logistic regression fitting
   - Range: [-1, 1], higher is better
2. **SRCC (Spearman Rank Correlation Coefficient)**

   - Measures monotonic relationship
   - More robust to outliers than PLCC
   - Range: [-1, 1], higher is better
3. **KRCC (Kendall Rank Correlation Coefficient)**

   - Alternative rank-based correlation
   - Measures ordinal association
   - Range: [-1, 1], higher is better

### Error Metrics

- **MSE**: Mean Squared Error
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error

### Nonlinear Regression

For PLCC calculation, the system can apply a 5-parameter logistic function:

```
f(x) = Î²â‚‚ + (Î²â‚ - Î²â‚‚) / (1 + exp(-(x - Î²â‚ƒ) / |Î²â‚„|)) + Î²â‚…Â·x
```

This accounts for nonlinear mapping between predicted scores and subjective ratings.

---

## ğŸ† Results

Results will vary based on:

- Dataset used
- Number of training epochs
- Model architecture
- Hyperparameters

Example results structure (after evaluation):

```markdown
# Experiment summary

## Identification:
- Dataset: `live`
- Config: `live_vit_base_patch16_224_baseline`
- Split: `test`
- Checkpoint: `last.pth`
- Device: `cuda`
- Number of samples: 64

## Correlation metrics:
- PLCC: `0.9234`
- SRCC: `0.9156`
- KRCC: `0.7891`

## Error metrics:
- MSE: 0.0234
- RMSE: 0.1530
- MAE: 0.1123
```

---

## ğŸ”§ Technical Details

### Data Splitting Strategy

To prevent data leakage, the system splits data **by reference images**:

1. Shuffle reference images deterministically (using random seed)
2. Split references into train/val/test
3. Assign all distortions of each reference to the same split

This ensures that distortions of the same reference never appear in different splits.

### Image Preprocessing

1. **Resize**: Images resized to 224Ã—224
2. **Aspect Ratio**: Option to preserve aspect ratio (with padding)
3. **Normalization**: ToTensor() converts to [0, 1] range
4. **Padding**: Black padding (0, 0, 0) for aspect ratio preservation

### Quality Score Normalization

- **MOS â†’ Unified**: `(value - min) / (max - min)`
- **DMOS â†’ Unified**: `1 - ((value - min) / (max - min))`
- All unified scores in [0, 1] where 1 = best quality

### Checkpoint Structure

Checkpoints contain:

```python
{
    'epoch': int,
    'model_state_dict': OrderedDict,
    'optimizer_state_dict': OrderedDict,
    'train_loss': float,
    'validation_loss': float
}
```

---

## ğŸ“¦ Requirements

### Core Dependencies

- **torch** - Deep learning framework
- **torchvision** - Computer vision utilities
- **timm** - Pretrained vision models
- **numpy** - Numerical computing
- **pandas** - Data manipulation
- **scipy** - Scientific computing (for metrics)
- **Pillow** - Image processing
- **scikit-image** - Image processing
- **opencv-python** - Computer vision
- **scikit-learn** - Machine learning metrics
- **einops** - Tensor operations
- **safetensors** - Safe tensor serialization
- **pyyaml** - Configuration parsing

### Development Dependencies

- **matplotlib** - Plotting
- **tqdm** - Progress bars
- **tensorboard** - Visualization

### System Requirements

- **Python**: 3.12.x
- **GPU**: CUDA-capable (recommended)
- **RAM**: 16GB+ recommended
- **Storage**: Depends on datasets (KADID-10k ~10GB, others smaller)

---

## ğŸ—ºï¸ Roadmap

### Upcoming Features

- [ ] Automatic dataset downloading
- [ ] Enhanced checkpoint management
- [ ] Early stopping implementation
- [ ] Extended logging to `train.log`
- [ ] Experiment consistency verification module
- [ ] Support for no-reference IQA
- [ ] Additional backbone architectures
- [ ] Hyperparameter optimization tools

---
